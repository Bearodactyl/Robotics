{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gtbook/robotics/blob/main/S76_drone_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JoW4C_OkOMhe",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%pip install -q -U gtbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10-snNDwOSuC",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import PIL\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import gtbook.stonehenge as stonehenge\n",
    "\n",
    "WHITE = torch.tensor([1,1,1], dtype=torch.float32 )\n",
    "BLUE = torch.tensor([0,0,1], dtype=torch.float32 )\n",
    "\n",
    "DEVICE = (\n",
    "    torch.device(\"cuda\") if torch.cuda.is_available()\n",
    "    # else torch.device(\"mps\") if torch.backends.mps.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAvx4-UCNzt2"
   },
   "source": [
    "# Neural Radiance Fields for Drones\n",
    "\n",
    "> Learning 3D scene representations from images.\n",
    "\n",
    "<img src=\"Figures7/S76-Autonomous_camera_drone-06.jpg\" alt=\"Splash image with a drone-like robot, steampunk style\" width=\"40%\" align=center style=\"vertical-align:middle;margin:10px 0px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a NeRF?\n",
    "\n",
    "> NeRFs represent 3D scenes to render new views.\n",
    "\n",
    "In the previous section we have seen how a density field can assist in planning drone trajectories while avoiding obstacles. In this section we will examine how to learn 3D representations from image data.\n",
    "\n",
    "A **neural radiance field** or \"NeRF\" is a neural representation of a 3D scene, and can be useful for drones to help with motion planning, obstacle avoidance, or even simply simulation of drone flights. NeRFs are a novel, data-driven solution to the long-standing problem in computer graphics of the realistic rendering of virtual worlds. They were introduced in the field of computer vision in 2020 by a team of researchers from Berkeley and Google, and have since seen an explosion of interest, including in the field of robotics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure id=\"fig:NeRF-setup\"> \n",
    "<img src=\"https://raw.githubusercontent.com/gtbook/robotics/main/Figures7/NeRF-setup.png?raw=1\" style=\"width:16cm\" alt=\"\">\n",
    "<figcaption>Figure 1: A NeRF stores a volumetric scene representation as the weights of an MLP, trained on many images with known pose.</figcaption>\n",
    "</figure>\n",
    "\n",
    "The original paper appeared at the European Conference on Computer Vision in 2020, and is cited below:\n",
    "\n",
    "> **[NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis](https://doi.org/10.48550/arXiv.2003.08934)**,\tBen Mildenhall*, Pratul Srinivasan*, Matthew Tancik*, Jonathan Barron, Ravi Ramamoorthi, Ren Ng, ECCV 2020.\n",
    "\n",
    "Figure <a href=\"#fig:NeRF-setup\" data-reference-type=\"ref\" data-reference=\"fig:NeRF-setup\">1</a>, from that paper, describes the setup: from a set of input images, with *known* camera pose and intrinsics calibration, a scene representation is learned that encodes the scene, which can then be used to render new views.\n",
    "The original NeRF paper defines a large neural network that can be trained to predict the value of every pixel in every image. By doing so, the neural network can then also *generate* new images that are not in the original training set. What is more, the neural network can also be used to predict the 3D structure of the underlying scene, making it possible to do much more than simply view synthesis.\n",
    "\n",
    "The original NeRF work has spawned a minor revolution in how 3D scenes are represented in the fields of computer graphics and computer vision, because:\n",
    "\n",
    "- the proposed scheme of learning a neural representation of the 3D scene was very simple;\n",
    "- the resulting NeRFs were capable of generating very realistic \"renderings\" of the learned scene.\n",
    "\n",
    "While the original NeRF paper was rather slow, because of the large neural network, since then faster *voxel-based* versions have been developed. We work with those variants, making it possible to train your own NeRF-like representation within a notebook.\n",
    "\n",
    "In particular, below we extend the 1D interpolation example from Chapter 5, extend it to interpolating in 3D voxel space, and show that this can be used to create a neural *radiance* field. However, we first need to talk about volume rendering, the basic math underlying all of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volume Rendering\n",
    "\n",
    "> Integrating color over a volume, mediated by density.\n",
    "\n",
    "The key technique in NeRF is **volume rendering**, a class of methods that generate images by tracing a ray into the scene and taking an integral of some sort over the length of the ray. In the original NeRF, a multi-layer perceptron is used to encode a function from the 3D coordinates on the ray to quantities like density and color, which are integrated to yield an image.\n",
    "\n",
    "<figure id=\"fig:NeRF-pipeline\"> \n",
    "<img src=\"https://raw.githubusercontent.com/gtbook/robotics/main/Figures7/NeRF-pipeline.png?raw=1\" style=\"width:16cm\" alt=\"\">\n",
    "<figcaption>Figure 2: New views are rendered by integrating the density and color at regular intervals along each viewing ray.</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Figure <a href=\"#fig:NeRF-pipeline\" data-reference-type=\"ref\" data-reference=\"fig:NeRF-pipeline\">2</a> sketches out how volume rendering works. On the left, for *every* pixel you want to render, a ray is projected into the 3D volume of interest (here represented by a cube) and sampled 3D point locations are defined along this ray. For every sample, the neural network $F_\\Theta$ is queried for both a local *density* $\\delta$ and a *color* $c$. The colors are then integrated along the ray, their contributions mediated by the local density, to yield a final pixel color.\n",
    "\n",
    "To get some intuition, think about where high density regions will exist for the simple example scene shown. Here, we expect the density to be high on the *surface* of the object - the lego excavator toy in this example. If that is the only high-density area on the ray, the pixel corresponding to that ray will just use the color from that high-density surface. However, the integration scheme, which we actually implement below, is also able to account for occlusions or even semi-transparency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from Rays\n",
    "\n",
    "The first part of a volume renderer is to evenly sample points on rays shot from optical center of a camera. If we are given the `origin` and the `direction` of a ray (as a unit-vector) sampling is simple enough: a point $P$ on the ray at a distance $t$ from the origin $O$, in the direction $D$ is given as\n",
    "\n",
    "$$\n",
    "P(t,O,D) = O + t  D\n",
    "$$\n",
    "\n",
    "where $t$ is a random depth along the ray. Hence, we only need to sample many $t$ values evenly and we are done. The `sample_along_ray` value takes a set of t_values and produces the corresponding sampled points $P$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_along_ray(t_values, origins, directions):\n",
    "    \"\"\"Sample points along rays defined by origins and (unit-norm) directions.\"\"\"\n",
    "    return origins[..., None, :] + t_values[:, None] * directions[..., None, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we took care to implement it so it can handle *arbitrary* batches of origin/direction pairs, as long as their last dimensions are 3. PyTorch broadcasting rules will take care to appropriately expand the array of t values. To illustrate this, we sample two rays at the same time in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_values = torch.linspace(0.0, 1.0, 25) # 25 points along each ray\n",
    "origins = torch.tensor([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]) # same origin.\n",
    "directions = torch.tensor([[1.0, 0.0, 0.0], [np.cos(0.3), np.sin(0.3), 0.0]])\n",
    "\n",
    "samples = sample_along_ray(t_values, origins, directions)\n",
    "\n",
    "assert samples.shape == torch.Size([2, 25, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last line above asserts that we sampled 2 rays for 25 different $t$-values, for a total of 50 three-dimensional points. We can also plot them, projected onto the xy-plane, to show visually what happened:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=px.scatter(x=samples[...,0].flatten(), y=samples[...,1].flatten())\n",
    "fig.add_scatter(x=[0], y=[0], mode='markers', marker=dict(color='red', size=10), name='Origin')\n",
    "fig.update_layout(yaxis=dict(scaleanchor=\"x\", scaleratio=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration along Rays\n",
    "\n",
    "The second part of the volume rendering story is where the magic happens. Let us assume that we are given the densities $\\sigma_i$ and colors $c_i$ at $N$ sampled points $P_i$ on a ray corresponding to a given pixel. We can then calculate the color $C$ for the pixel using the equation below,\n",
    "\n",
    "$$\n",
    "C = \\sum_{i=1}^N \\alpha_i T_i c_i\n",
    "$$\n",
    "\n",
    "where $\\alpha_i$ is the **opacity** and $T_i$ is the **transmittance** at $P_i$, both defined below:\n",
    "\n",
    "$$\n",
    "\\alpha_i \\doteq 1 - \\exp(-\\sigma_i) \\\\\n",
    "T_i \\doteq \\exp ( - \\sum_{j=1}^{i-1} \\sigma_j).\n",
    "$$\n",
    "\n",
    "The quantity $\\alpha_i$ is the easier of the two to understand: a density $\\sigma=0$ corresponds to an alpha value $\\alpha=0$, signifying full transparency. Conversely, a high density $\\sigma >> 0$ will yield an alpha value close to $\\alpha=1$, i.e., fully opaque. The transmittance $T_i$, on the other hand, is computed over all samples up to the current sample $P_i$, and measures the *lack* of occlusion in the space between the $i^th$ sample and the ray origin. \n",
    "\n",
    "The calculation of $C$ above simulates how light is absorbed as it travels through a medium, and can even deal with semi-transparency such as smoke, flames, frosted windows, etc. It is a simplified version of the calculation in the original NeRF paper as we do not bother to deal with non-uniform sampling. The color $c_i$ at the $i^th$ sample will contribute a lot if there is not a lot of \"stuff\" between it and the origin (transmittance $T_i$ is high), *and* the local opacity $\\alpha_i$ is high. Conversely, if either visibility from the origin is occluded (transmittance $T_i$ is low) *or* there is really nothing there (opacity $\\alpha_i$ is low) then the sample will not contribute much to the pixel color $C$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `render_along_ray` below implements this equation, again taking care that arbitrary batch dimensions are correctly handled. It also allows for specifying a background color that is composited with the rendered color in case the ray does not hit anything in the scene. We defined the default color `WHITE` in the preamble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_along_ray(density, rgb, background=WHITE):\n",
    "    \"\"\"Compute the final rendered color given the density and RGB values.\"\"\"\n",
    "    alpha = 1 - torch.exp(-density)\n",
    "    cumulative_density = torch.cumsum(density, dim=-1)\n",
    "    trans = torch.exp(-cumulative_density)\n",
    "    trans = torch.cat([torch.ones_like(density[..., :1]), trans[..., :-1]], dim=-1)\n",
    "\n",
    "    weights = alpha * trans\n",
    "    color_acc = torch.einsum('...i,...ij->...j', weights, rgb)\n",
    "    acc = weights.sum(dim=-1, keepdim=True)\n",
    "\n",
    "    return color_acc + (1.0 - acc) * background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We illustrate this volume rendering code below, using randomly generated `density` and `rgb` inputs that have the same shape as our sampled rays from above, demonstrating that we indeed get *two* RGB colors as the end-result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "density = torch.rand(2, 5) # Random density\n",
    "rgb = torch.rand(2, 5, 3) # Random colors (between 0 and 1)\n",
    "rendered = render_along_ray(density, rgb)\n",
    "assert rendered.shape == torch.Size([2, 3])\n",
    "print(rendered.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, because our inputs are random the result is also totally random! To remedy that we add the final piece of the puzzle below: a way to regress the densities and colors in a given volume from a set of input images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Differentiable Voxel Grid\n",
    "\n",
    "> A voxel grid of density or color.\n",
    "\n",
    "A key decision in the NeRF architecture is how to represent the function $F_\\Theta$ so it can be queried for a density and a color at every point. In short, we need:\n",
    "\n",
    "$$\n",
    "(\\sigma, c) = F_\\Theta(P)\n",
    "$$\n",
    "\n",
    "with $P\\in R^3$, $\\sigma\\in R$, and $c\\in[0...255]^3$. Note that in computer vision, neural networks typically take high-dimensional inputs, e.g., entire images, so having such a low-dimensional input (3) is highly unusual. This \"coordinate-based\" approach is one of the breakthroughs of NeRF-style representations.\n",
    "\n",
    "We will use a **voxel grid** to represent $F_\\Theta$. In the original NeRF paper, the authors used a fully connected neural network, also called a multi-layer perceptron (MLP) that takes a 3D coordinate as input, and in some variants also a ray direction. In DVGO, a later and much faster method, the color and density are instead stored in a voxel grid, which is the approach we adopt. The key is to create a *differentiable* 3D interpolation scheme, that we can then *train* by asking it to predict the colors in the input images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build upon the 1D interpolation example that we introduced in Section 5.6. However, to represent 3D scenes we need to generalize it to 3D. In particular, the interpolation function has to be changed from simple linear interpolation over an interval, to **trilinear interpolation** on voxels. \n",
    "The code below implements trilinear interpolation: the values on the *eight* corners of each voxel are combined using three blending weights, which depend on where the queried point lies within the voxel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bracket(x, n):\n",
    "    \"\"\"Return the indices of the nearest grid points to x, as well as weights.\"\"\"\n",
    "    # Check device of x and n:\n",
    "    x0 = torch.floor(x).long()\n",
    "    X0 = torch.clamp(x0, min=0, max=(n - 1))\n",
    "    X1 = torch.clamp(x0 + 1, min=0, max=(n - 1))\n",
    "    return X0, X1, torch.clamp(x - x0.float(), min=0.0, max=1.0)\n",
    "\n",
    "def interpolate(v0, v1, alpha):\n",
    "    \"\"\"Interpolate between v0 and v1 using alpha, using unsqueeze to properly handle batches.\"\"\"\n",
    "    return v0 * (1 - alpha.unsqueeze(-1)) + v1 * alpha.unsqueeze(-1)\n",
    "\n",
    "class VoxelGrid(nn.Module):\n",
    "    def __init__(self, shape, d=1, max=1.0, fill=None):\n",
    "        \"\"\"A 3D voxel grid with given `shape` with learnable values at the middle of the voxels.\"\"\"\n",
    "        super(VoxelGrid, self).__init__()\n",
    "        # Note that we store *corner* values, so we need one more point in each dimension:\n",
    "        storage_shape = tuple(s + 1 for s in shape)\n",
    "        if fill is not None:\n",
    "            self.grid = nn.Parameter(fill.view(1, 1, 1, d).repeat(*storage_shape, 1))\n",
    "        else:\n",
    "           self.grid = nn.Parameter(torch.rand(*storage_shape, d, dtype=torch.float32) * max)\n",
    "\n",
    "    def forward(self, P):\n",
    "        \"\"\"Implement trilinear interpolation at the points P.\"\"\"\n",
    "        x, y, z = P[..., 0], P[..., 1], P[..., 2]\n",
    "\n",
    "        # Get indices of the corners, clamping to the grid size where needed:\n",
    "        X0, X1, a = bracket(x, self.grid.shape[0])\n",
    "        Y0, Y1, b = bracket(y, self.grid.shape[1])\n",
    "        Z0, Z1, c = bracket(z, self.grid.shape[2])\n",
    "\n",
    "        # Interpolate in the x direction:\n",
    "        y0z0 = interpolate(self.grid[X0, Y0, Z0, :], self.grid[X1, Y0, Z0, :], a)\n",
    "        y1z0 = interpolate(self.grid[X0, Y1, Z0, :], self.grid[X1, Y1, Z0, :], a)\n",
    "        y0z1 = interpolate(self.grid[X0, Y0, Z1, :], self.grid[X1, Y0, Z1, :], a)\n",
    "        y1z1 = interpolate(self.grid[X0, Y1, Z1, :], self.grid[X1, Y1, Z1, :], a)\n",
    "\n",
    "        # Interpolate in the y direction:\n",
    "        z0 = interpolate(y0z0, y1z0, b)\n",
    "        z1 = interpolate(y0z1, y1z1, b)\n",
    "\n",
    "        # Interpolate in the z direction:\n",
    "        return interpolate(z0, z1, c).squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VoxelGrid effectively defines a parameterized function in 3D. When we query it, we need to provide 3D coordinates. For example, the code below initializes a VoxelGrid with random values, and then evaluates the a scalar function at a 3D point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel_grid_module = VoxelGrid(shape=(6, 6, 6), d=1)\n",
    "point = torch.Tensor([1.5, 2.7, 3.4])\n",
    "output = voxel_grid_module(point)\n",
    "print(f\"Interpolated Output: {output.item():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again the code is carefully written so it can handle both arbitrary batches inputs and multi-dimensional outputs. As an example, below we create a grid that interpolates a three-dimensional function (`d=3`), like color, and evaluate it at a 2x2 batch `x` of 3D points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel_grid_module = VoxelGrid(shape = (6, 6, 6), d=3, fill=BLUE)\n",
    "x = torch.Tensor([[[1.5, 2.7, 3.4], [2.3, 4.6, 1.1]], [[2.3, 4.6, 1.1], [2.3, 4.6, 1.1]]])\n",
    "y = voxel_grid_module(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", y.shape)\n",
    "print(\"Interpolated Output:\\n\", y.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we used a variant of the constructor that fills the grid with an initial value, here the color `BLUE` defined in the preamble. Being able to handle large batches of points is crucial when training with stochastic gradient descent, as we saw before. But when training a NeRF, we will also use the ability to have arbitrary input shapes for other purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DVGO\n",
    "\n",
    "We can now define a simple DVGO ([Direct Voxel Grid Optimization, Sun et al., CVPR 2022](https://doi.org/10.48550/arXiv.2111.11215)) class that combines:\n",
    "\n",
    "- sampling points $P$ along each ray;\n",
    "- converting from scene coordinates into grid coordinates;\n",
    "- retrieving density and color by interpolating in *two* voxel grids;\n",
    "- volume rendering.\n",
    "\n",
    "Each of these steps has a number of configurable parameters which you can experiment with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    near: float = 1.5\n",
    "    far: float = 3.5\n",
    "    num_samples: int = 64\n",
    "    min_corner: tuple[float] = (-1.0, -1.0, 0.0)\n",
    "    max_corner: tuple[float] = (1.0, 1.0, 1.0)\n",
    "    shape: tuple[int] = (128, 128, 128)\n",
    "    background = WHITE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DVGO class below first pre-computes a number of things in the constructor, and then just has a fully differentiable `forward` method. Beyond beyond sampling rays and volume rendering, which we discussed before, there are three new pieces:\n",
    "\n",
    "- there is a rescaling step to go from scene to grid coordinates\n",
    "- the output from the density voxel grid is put through a softplus nonlinearity.\n",
    "- the color from the density voxel grid is put through a sigmoid nonlinearity.\n",
    "\n",
    "The latter two steps are a differentiable way to ensure the output samples remain within their expected ranges. Note also that training a NeRF can take a long time, so we are careful to use `float32` everywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_rays(t_values, rays, training=True):\n",
    "    \"\"\"Sample points along the rays, using the t_values defined in the constructor.\n",
    "        During training, add a small random scalar to t_values to prevent overfitting to the\n",
    "        discrete sampling locations.\n",
    "    \"\"\"\n",
    "    # Extract ray origins and directions from rays\n",
    "    origins = rays[..., :3].to(dtype=torch.float32)\n",
    "    directions = rays[..., 3:].to(dtype=torch.float32)\n",
    "\n",
    "    # Add a small random scalar to t_values during training\n",
    "    if training:\n",
    "        with torch.no_grad():\n",
    "            n = t_values.size(0)\n",
    "            random_scalar = (torch.rand(n, device=rays.device) - 0.5) / n\n",
    "            actual_ts = t_values.clone() + random_scalar\n",
    "    else:\n",
    "        actual_ts = t_values.clone()\n",
    "\n",
    "    # Sample along the ray\n",
    "    return sample_along_ray(actual_ts, origins, directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDVGO(nn.Module):\n",
    "    def __init__(self, config: Config = Config()):\n",
    "        \"\"\"Initialize voxel grids and bounding box corners.\"\"\"\n",
    "        super().__init__()  # Calling the superclass's __init__ method\n",
    "\n",
    "        # Initialize sampler parameters:\n",
    "        self.depths = torch.linspace(config.near, config.far, config.num_samples + 1,\n",
    "                                     dtype=torch.float32)\n",
    "        self.register_buffer('t_values', 0.5 * (self.depths[1:] + self.depths[:-1]))\n",
    "\n",
    "        # Set up conversion from scene coordinates to grid coordinates:\n",
    "        self.register_buffer('min', torch.tensor(config.min_corner, dtype=torch.float32))\n",
    "        self.register_buffer('max', torch.tensor(config.max_corner, dtype=torch.float32))\n",
    "        self.register_buffer('shape', torch.tensor(config.shape, dtype=torch.float32))\n",
    "\n",
    "        # Initialize color grid to blue:\n",
    "        self.rgb_voxel_grid = VoxelGrid(config.shape, d=3, fill=BLUE)\n",
    "\n",
    "        # Initialize density grid with very low density:\n",
    "        self.density_voxel_grid = VoxelGrid(config.shape, d=1, fill=torch.full((), 0.001, dtype=torch.float32))\n",
    "\n",
    "        # Finally, record background color for rendering:\n",
    "        self.register_buffer('background', config.background)\n",
    "\n",
    "    def forward(self, rays, training=True):\n",
    "        \"\"\"Perform volume rendering using the provided ray information.\"\"\"\n",
    "        samples = sample_rays(self.t_values, rays, training=training)\n",
    "\n",
    "        # Rescale to fit within the grid\n",
    "        rescaled = self.shape * (samples - self.min) / (self.max - self.min)\n",
    "\n",
    "        # Query Density Voxel Grid\n",
    "        density = torch.squeeze(self.density_voxel_grid(rescaled))\n",
    "        density = F.relu(density)\n",
    "\n",
    "        # Query RGB Voxel Grid\n",
    "        rgb = torch.clamp(self.rgb_voxel_grid(rescaled), 0.0, 1.0)\n",
    "\n",
    "        # Render\n",
    "        return render_along_ray(density, rgb, self.background)\n",
    "\n",
    "    def alpha(self):\n",
    "        \"\"\"return the alpha for the density voxel grid\"\"\"\n",
    "        density = F.relu(self.density_voxel_grid.grid)\n",
    "        return 1 - torch.exp(-density)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give it a whirl below, calculating the colors for 32 random rays, each with their origin and direction stacked into a 6-vector, so the input batch size is $32 \\times 6$, and we expect an output batch size of RGB colors, i.e., $32 \\times 3$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize renderer\n",
    "dvgo = SimpleDVGO()\n",
    "\n",
    "x_samples = torch.rand((32, 6)).to(DEVICE)\n",
    "y_samples = dvgo(x_samples)\n",
    "# Verify shape of the output\n",
    "print(\"Output tensor shape:\", y_samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simulated dataset\n",
    "\n",
    "<figure>\n",
    "<img src=\"stonehenge/train/render47.png\" id=\"fig:stonehenge\" style=\"width:14cm\" alt=\"\">\n",
    "<figcaption>One of the synthetically generated images from the stonehenge dataset.</figcaption>\n",
    "</figure>\n",
    "\n",
    "We will use a dataset generated by Stanford researchers using Blender, using a simple 3D model of the Stonehenge monument in England. The dataset was originally published as part of the [Stanford NeRF Navigation project](https://mikh3x4.github.io/nerf-navigation/), and consists of 500 images, split into 200 images for training, and then validation and tests sets of 150 images each. One of the training images is shown above. The code below illustrates how to read the image into memory and display it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_training_image(index:int):\n",
    "    \"\"\"Read image from the stonehenge dataset, and return as a PIL image.\"\"\"\n",
    "    # url = f\"https://raw.githubusercontent.com/gtbook/robotics/main/Figures7/{image_name}\"\n",
    "    # return PIL.Image.open(requests.get(url, stream=True).raw)\n",
    "    image = PIL.Image.open(f\"stonehenge/train/render{index}.png\")\n",
    "    # Blend image onto white background\n",
    "    rgb_image = PIL.Image.new('RGB', image.size, (255, 255, 255))\n",
    "    rgb_image.paste(image, mask=image.split()[3])  # 3 is the index of the alpha channel\n",
    "    return rgb_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = read_training_image(23)\n",
    "plt.imshow(image);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to modify the index above and examine any of the other 198 images in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a NeRF\n",
    "\n",
    "Now that we built the basic infrastructure to render a NeRF and regress it from data, let us apply it to real images. To do this, we will need two distinct pieces of information:\n",
    "\n",
    "- the actual images themselves\n",
    "- accurate geometry from where the images were taken\n",
    "\n",
    "The latter is important because, as we saw above, a NeRF is trained with a set of *rays*. For a given image, every pixel in the image corresponds to a ray, and the *origin* of the ray is exactly the optical center of the camera. To calculate the *direction* of the ray need two pieces of information for ech image:\n",
    "\n",
    "- the *intrinsic* calibration of the camera, most importantly the focal length, tells us how to convert pixel coordinates into a direction in the camera coordinate frame.\n",
    "- the *extrinsic* calibration, position and orientation with which the image was taken, is needed to transform directions in the camera frame into the scene coordinate frame.\n",
    "\n",
    "Acquiring all this information for an arbitrary image sequence taken with some unknown, uncalibrated camera can be complicated. Cameras come in a variety of sizes and with very different lenses, and effects like radial distortion make the modeling process non-trivial. In addition, recovering the actual position and attitude of the camera in a scene is typically done through structure from motion, which can be time-consuming and tricky."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the following, we will assume that all this hard work has been done for us, and/or the images have been simulated with exactly known camera parameters, both intrinsic and extrinsic. One popular way to accomplish this is by providing undistorted images accompanied by a $3 \\times 4$ **camera matrix** $M$. Recall that a 3D point $P$ can be projected into an image via\n",
    "\n",
    "$$\n",
    "\\tilde{p} = K R^T (P - t)\n",
    "$$\n",
    "\n",
    "where $\\tilde{p}$ are *homogeneous* 2D image coordinates. We can re-write this as\n",
    "\n",
    "$$\n",
    "\\tilde{p} = M\\tilde{P}\n",
    "$$\n",
    "\n",
    "where $\\tilde{P} = \\begin{bmatrix}P \\\\1 \\end{bmatrix}$ and the camera matrix $M$ is given by\n",
    "\n",
    "$$\n",
    "M = [A|a] = [K R^T | - K R^T t]\n",
    "$$\n",
    "\n",
    "That means that if we are *given* the camera matrix $M$ we can always recover the ray origins as \n",
    "$$\n",
    "t = -A^{-1} a\n",
    "$$\n",
    "\n",
    "and a random 3D point $P$ on the ray corresponding to $\\tilde{p}$ as\n",
    "\n",
    "$$\n",
    "P = A^{-1}(\\tilde{p} - a)\n",
    "$$\n",
    "\n",
    "since $\\tilde{p} = AP + a$. We will illustrate this below with the Stonehenge dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Rays\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Stonehenge dataset came with its camera matrices: they were all written in a json file, which we can parse into a python dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the local JSON file and read its content\n",
    "data = data = stonehenge.load_json(\"transforms_train.json\")\n",
    "\n",
    "# Now, `data` contains the parsed JSON content.\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The camera matrix associated with the image below can then be extracted by converting to numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = stonehenge.extract_camera_matrix(data, 47, (200,200))\n",
    "print(M.shape)\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then calculate the ray origin and ray direction for any pixel. For expediency's sake, this is done in the `gtbook.stonehenge` library for us, and we can download the rays directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_samples, y_samples = stonehenge.download_rays(M=199)\n",
    "\n",
    "# load x_samples and y_samples from disk\n",
    "with np.load('stonehenge/training_rays-199-4.npz') as data:\n",
    "    x_samples = data['x']\n",
    "    y_samples = data['y']\n",
    "\n",
    "# check that they have the right shape:\n",
    "assert x_samples.shape == (199, 200, 200, 6)\n",
    "assert y_samples.shape == (199, 200, 200, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the tensors and create a dataset\n",
    "x_view = torch.from_numpy(x_samples).view(-1, x_samples.shape[-1])\n",
    "y_view = torch.from_numpy(y_samples).view(-1, y_samples.shape[-1])\n",
    "dataset = TensorDataset(x_view, y_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us show what a batch of 100 sampled rays looks like, with the rendering volume shown as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size=300, shuffle=True)\n",
    "data_iter = iter(data_loader)\n",
    "x_batch, y_batch = next(data_iter)\n",
    "assert x_batch.shape == torch.Size([300, 6])\n",
    "assert y_batch.shape == torch.Size([300, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "for i in range(300):\n",
    "    T = x_batch[i,:3]\n",
    "    D = x_batch[i,3:]\n",
    "    # Print devices for three params:\n",
    "    samples = sample_along_ray(dvgo.t_values.cpu(), T, D)\n",
    "\n",
    "    # Adding line segments for each ray\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=samples[:, 0],\n",
    "            y=samples[:, 1],\n",
    "            z=samples[:, 2],\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=\"red\"),\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=[T[0].item()],\n",
    "            y=[T[1].item()],\n",
    "            z=[T[2].item()],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(color=\"red\", size=2),\n",
    "        )\n",
    "    )\n",
    "\n",
    "# add a cuboid from -1 to 1 in x and y, and 0 to 0.5 in z:\n",
    "fig.add_trace(\n",
    "    go.Mesh3d(\n",
    "        x=[-1, 1, 1, -1, -1, 1, 1, -1],\n",
    "        y=[-1, -1, 1, 1, -1, -1, 1, 1],\n",
    "        z=[-0.5, -0.5, -0.5, -0.5, 0.5, 0.5, 0.5, 0.5],\n",
    "        i=[0, 0, 4, 4],\n",
    "        j=[1, 2, 5, 6],\n",
    "        k=[2, 3, 6, 7],\n",
    "        opacity=0.5,\n",
    "        color=\"lightblue\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(showlegend=False, margin=dict(l=0, r=0, b=0, t=0))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us recall the training code from Chapter 5. We previously used this code to train for the parameters of a 1D interpolation grid, minimizing a Mean-Squared Error (MSE) loss function, i.e., the squared difference between the predicted values and the training data values. This is the standard loss function for continuous *regression* problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sgd(model, dataset, callback=None, learning_rate=0.01, num_epochs=10, batch_size=4096):\n",
    "    # Initialize optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create DataLoader for batch processing\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Initialize the built-in Mean-Squared Error loss function\n",
    "    mse = nn.MSELoss()\n",
    "\n",
    "    # Loop over the dataset multiple times (each loop is an epoch)\n",
    "    iteration = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for x_batch, y_batch in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x_batch.to(DEVICE))\n",
    "            loss = mse(y_pred, y_batch.to(DEVICE))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if callback is not None:\n",
    "                callback(epoch, iteration, loss.item())\n",
    "\n",
    "            iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your SimpleDVGO model\n",
    "base = 256\n",
    "config = Config(\n",
    "    near=1.0,\n",
    "    far=4.0,\n",
    "    num_samples=base,\n",
    "    min_corner=(-1.5, -1.5, -1.5),\n",
    "    max_corner=(1.5, 1.5, 1.5),\n",
    "    shape=(base-1, base-1, base-1)\n",
    ")\n",
    "\n",
    "model = SimpleDVGO(config).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a callback that saves loss to a dataframe\n",
    "loss_data = pd.DataFrame(columns=['Iteration', 'Loss'])\n",
    "def record_loss(epoch:int, iteration:int, loss:float):\n",
    "    if iteration % 100 != 0:\n",
    "        return\n",
    "    print(f\"Epoch: {epoch}, iteration {iteration}: {loss:.5f}\")\n",
    "    loss_data.loc[len(loss_data)] = [iteration, loss]\n",
    "\n",
    "# Run the training loop\n",
    "train_sgd(model, dataset, callback=record_loss, num_epochs=10, batch_size=1024, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(loss_data, x='Iteration', y='Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3; predicted = model(torch.from_numpy(x_samples[i]), training=False)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(y_samples[i])\n",
    "ax[0].set_title(\"Ground Truth\")\n",
    "ax[1].imshow(predicted.detach().numpy())\n",
    "ax[1].set_title(\"Predicted\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show every 10th image in a 5x4 grid:\n",
    "fig, ax = plt.subplots(4, 5, figsize=(20, 16))\n",
    "for i in range(20):\n",
    "    predicted = model(torch.from_numpy(x_samples[i*10]), training=False)\n",
    "    ax[i//5, i%5].imshow(predicted.detach().numpy())\n",
    "    ax[i//5, i%5].set_title(f\"Predicted {i*10}\")\n",
    "    ax[i//5, i%5].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(model.rgb_voxel_grid.grid[:,:,30].detach().numpy());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(torch.sum(model.alpha(), axis=0).detach().numpy());\n",
    "# add legend:\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show every 10th alpha slice in a 5x4 grid:\n",
    "fig, ax = plt.subplots(4, 5, figsize=(20, 16))\n",
    "alpha = model.alpha().detach().numpy()\n",
    "dd = 128 // 20\n",
    "for i in range(20):\n",
    "    ax[i//5, i%5].imshow(alpha[:,:,i*dd])\n",
    "    ax[i//5, i%5].set_title(f\"Alpha {i*dd}\")\n",
    "    ax[i//5, i%5].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "S76_drone_learning.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "latex_metadata": {
   "affiliation": "Georgia Institute of Technology",
   "author": "Frank Dellaert and Seth Hutchinson",
   "title": "Introduction to Robotics"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
