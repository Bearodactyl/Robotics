{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gtbook/robotics/blob/main/S76_drone_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JoW4C_OkOMhe",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%pip install -q -U gtbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10-snNDwOSuC",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import plotly\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAvx4-UCNzt2"
   },
   "source": [
    "# Neural Radiance Fields for Drones\n",
    "\n",
    "<img src=\"Figures7/S76-Autonomous_camera_drone-07.jpg\" alt=\"Splash image with a drone-like robot, steampunk style\" width=\"40%\" align=center style=\"vertical-align:middle;margin:10px 0px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a NeRF?\n",
    "\n",
    "In the previous section we have seen how a density field can assist in planning drone trajectories while avoiding obstacles. In this section we will examine how to learn such maps from data.\n",
    "\n",
    "A **neural radiance field** of \"NeRF\" is a neural representation of a 3D scene, and hence can be very useful for drones to help with motion planning, obstacle avoidance, or even simply simulation of drone flights. NeRFs were introduced in the field of computer vision in 2020 by a team of researchers from Berkeley and Google, and have since seen an explosion of interest. The reasons are two-fold:\n",
    "\n",
    "- their proposed scheme of learning a neural representation of the 3D scene was very simple\n",
    "- the resulting NeRFs were capable of generating very realistic \"renderings\" of the learned scene\n",
    "\n",
    "In a nutshell, given a large set of images taken of a 3D scene, the original NeRF trained a large (but simple) neural network to predict the value of every pixel in every image. By doing so, the neural network could then also *generate* new images that were not in the original training set. What is more, the neural network can also be used to predict the 3D structure of the underlying scene, making it possible to do much more than simply view synthesis.\n",
    "\n",
    "The original NeRF paper (Mildenhall et al., ECCV 2020) was rather slow, because of the large neural network used. Since then, however, faster *voxel-based* versions have been developed. In this chapter, we first introduce a 1D version of this basic scheme, then move on to 3D (voxels), and finally show how it can be used to create a neural *radiance* field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiable interpolation in 1D\n",
    "\n",
    "A neural radiance field will predict density and color in 3D, but we can start off simpler by just interpolating functions in 1D. The key is to create a *differentiable* interpolation scheme, that we can then *train* using samples from the function we want to interpolate.  The class LineGrid below does just that: it is initialized with two parameters:\n",
    " \n",
    " - `n` is how many cells the 1D grid has\n",
    " - `d` is the dimensionality of the function we want to interpolate\n",
    " \n",
    "Being able to learn multi-dimensional functions is crucial, as in a NeRF we will have to approximate RGB colors.\n",
    "\n",
    "In pytorch you also have to define a `forward` method when defining a module, after which you can \"call\" the module. In our case, we will simply interpolate the grid values defined at the cell boundaries, for any value inside the grid. We can give any value $x\\in[0,size]$, or even a tensor of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(v0, v1, alpha):\n",
    "    \"\"\"Interpolate between v0 and v1 using alpha, using unsqueeze to properly handle batches.\"\"\"\n",
    "    return v0 * (1 - alpha.unsqueeze(-1)) + v1 * alpha.unsqueeze(-1)\n",
    "\n",
    "class LineGrid(nn.Module):\n",
    "    \"\"\"A 1D grid with learnable values at the boundaries on n cells.\"\"\"\n",
    "    def __init__(self, n, d=1):\n",
    "        super(LineGrid, self).__init__()\n",
    "        self.grid = nn.Parameter(nn.init.normal_(torch.empty(n + 1, d)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        X = torch.floor(x).long()\n",
    "        a = x - X # blending weights (same size as x)\n",
    "        return interpolate(self.grid[X], self.grid[X+1], a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how to initialize a line grid and call the forward method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_module = LineGrid(n=5, d=2)\n",
    "\n",
    "x = torch.Tensor([1.5, 2.7, 3.6])\n",
    "print(\"Interpolated Output:\", grid_module(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, the shape of the output is $3\\times 2$ because we asked to interpolate a 2D function (`d` is 2) at 3 different locations (as `x.shape` is 3). The output looks rather random, however, because the grid was initialized with random values in the constructor. \n",
    "\n",
    "To \"learn\" a function we need to provide training data, and minimize a loss function. As an example, maybe we can learn a sine and cosine function at the same time? Let us create some training data by creating 500 samples of these two functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20 # we use a grid size of 20, allowing for x values between 0 and 20\n",
    "num_samples = 500 # we use 500 samples to train our model\n",
    "x_samples = torch.rand((num_samples,)) * n\n",
    "y_samples = torch.stack([torch.sin(x_samples * 2 * torch.pi / n) + 0.1 * torch.randn((num_samples, )), \n",
    "                         torch.cos(x_samples * 2 * torch.pi / n) + 0.1 * torch.randn((num_samples, ))], dim=1)\n",
    "print(y_samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training code below is a standard way of training a neural network using PyTorch, which we will abuse here to optimize instead for the parameters of a LineGrid. That is possible because all the operations inside our LineGrid class are differentiable, so stochastic gradient descent (SGD) will just work. The loss we will minimize is the **Mean-Squared Error** loss function or MSE, which minimized the squared difference between the predicted values and the training data values. This is the standard loss function for so-called *regression* problems, where we are trying to optimize a continuous function. \n",
    "\n",
    "Inside the training loop below, you'll find the typical sequence of operations: zeroing gradients, performing a forward pass to get predictions, computing the loss, and doing a backward pass to update the model's parameters. Try to understand the code, as we will use it as is later to learn 3D neural radiance fields, and in fact this same training loop is at the core of most deep learning architectures. Now, let's take a closer look at the code itself, which is extensively documented for clarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x_samples, y_samples, learning_rate=0.3, num_epochs=601, checkpoint_freq=100):\n",
    "    # Initialize Stochastic Gradient Descent optimizer\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Initialize the built-in Mean-Squared Error loss function\n",
    "    mse = nn.MSELoss()\n",
    "\n",
    "    # Loop over the dataset multiple times (each loop is an epoch)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: Compute predicted y by passing x_samples through the model\n",
    "        output = model(x_samples)\n",
    "\n",
    "        # Compute loss using built-in MSE loss function\n",
    "        loss = mse(output, y_samples)\n",
    "\n",
    "        # Backward pass: Compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters using optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss at specified checkpoint frequencies\n",
    "        if epoch % checkpoint_freq == 0:\n",
    "            print(f'Loss at epoch {epoch}: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this code is rather trivial: we initialize a grid with random values, and call `train`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = LineGrid(n=n, d=2)  # d=2 as we are regressing both sin and cos\n",
    "\n",
    "# Run the training loop\n",
    "train(model, x_samples, y_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then evaluate the resulting functions and plot the result against the training data, and we see that we get decent approximations of sin and cos, even with noisy training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(x_samples).detach().numpy()\n",
    "\n",
    "fig = plotly.graph_objects.Figure()\n",
    "fig.add_scatter(x=x_samples, y=y_samples[:, 0], mode='markers', name='sin')\n",
    "fig.add_scatter(x=x_samples, y=y_samples[:, 1], mode='markers', name='cos')\n",
    "fig.add_scatter(x=x_samples, y=y_pred[:, 0], mode='markers', name='predicted sin')\n",
    "fig.add_scatter(x=x_samples, y=y_pred[:, 1], mode='markers', name='predicted cos')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Differentiable Voxel Grid\n",
    "\n",
    "The scheme above can be easily extended to 3D voxel grids, although the interpolation function has to be changed from simple linear interpolation over an interval, to **trilinear interpolation** on voxels. The code below implements this, and it really is not that complicated: the values on the *eight* corners of each voxel are combined using three blending weights, depending on where the queried point lies within that voxel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoxelGrid(nn.Module):\n",
    "    def __init__(self, shape, d=1):\n",
    "        \"\"\"A 3D voxel grid with given `shape` with learnable values at the corners of the voxels.\"\"\"\n",
    "        super(VoxelGrid, self).__init__()\n",
    "        self.grid = nn.Parameter(nn.init.normal_(torch.empty(*shape, d)))\n",
    "\n",
    "\n",
    "    def forward(self, p):\n",
    "        x, y, z = p[..., 0], p[..., 1], p[..., 2]\n",
    "        X, Y, Z = torch.floor(x).long(), torch.floor(y).long(), torch.floor(z).long()\n",
    "        a, b, c = x - X, y - Y, z - Z # blending weights along each axis\n",
    "\n",
    "        # clamp indices to grid size:\n",
    "        Xp = torch.clamp(X + 1, max=self.grid.shape[0] - 1)\n",
    "        Yp = torch.clamp(Y + 1, max=self.grid.shape[1] - 1)\n",
    "        Zp = torch.clamp(Z + 1, max=self.grid.shape[2] - 1)\n",
    "\n",
    "        c00 = interpolate(self.grid[Z, Y, X, :], self.grid[Z, Y, Xp, :], a)\n",
    "        c01 = interpolate(self.grid[Z, Yp, X, :], self.grid[Z, Yp, Xp, :], a)\n",
    "        c10 = interpolate(self.grid[Zp, Y, X, :], self.grid[Zp, Y, Xp, :], a)\n",
    "        c11 = interpolate(self.grid[Zp, Yp, Xp, :], self.grid[Zp, Yp, X, :], a)\n",
    "\n",
    "        c0 = interpolate(c00, c01, b)\n",
    "        c1 = interpolate(c10, c11, b)\n",
    "        \n",
    "        return interpolate(c0, c1, c).squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VoxelGrid effectively defines a parameterized function in 3D. when we query it, we need to provide 3D coordinates. For example, the code below initializes a VoxelGrid with random values, and then evaluates the a scalar function at a 3D point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel_grid_module = VoxelGrid(shape=(6, 6, 6), d=1)\n",
    "\n",
    "point = torch.Tensor([1.5, 2.7, 3.4])\n",
    "output = voxel_grid_module(point)\n",
    "print(\"Interpolated Output:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the code is much more powerful than this. As an example, below we create a grid with a 4D function, and evaluate it at a 2x2 batch of 3d points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel_grid_module = VoxelGrid(shape = (6, 6, 6), d=4)\n",
    "\n",
    "points = torch.Tensor([[[1.5, 2.7, 3.4], [2.3, 4.6, 1.1]], [[2.3, 4.6, 1.1], [2.3, 4.6, 1.1]]])\n",
    "output = voxel_grid_module(points)\n",
    "print(\"Interpolated Output:\", output.shape)\n",
    "print(\"Interpolated Output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being able to handle large batches of points is crucial when training with stochastic gradient descent, and especially when training a NeRF, which we finally get to in the section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiable Rendering\n",
    "\n",
    "For NeRF, we need to add volume rendering.\n",
    "The Renderer class below is responsible for rendering 3D objects in a scene based on how light interacts with them. It contains two main functions: `sample_along_ray` and `render`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Renderer:\n",
    "    \"\"\"Class for rendering 3D scenes into 2D images.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_samples=32, near=0.0, far=1.0, background=(0., 0., 0.)):\n",
    "        \"\"\"Initialize renderer with parameters.\"\"\"\n",
    "        t_vals = torch.linspace(0., 1., num_samples + 1)\n",
    "        self.t_vals = near * (1. - t_vals) + far * t_vals\n",
    "        self.t_mids = 0.5 * (self.t_vals[1:] + self.t_vals[:-1])\n",
    "        self.background = background\n",
    "\n",
    "    def sample_along_ray(self, origins, directions):\n",
    "        \"\"\"Sample points along rays defined by origins and directions.\"\"\"\n",
    "        norms = torch.norm(directions, dim=-1, keepdim=True)\n",
    "        scaled_t_mids = self.t_mids * norms\n",
    "        depths = self.t_vals * norms\n",
    "        samples = origins[..., None, :] + scaled_t_mids[..., None] * directions[..., None, :]\n",
    "        return depths, samples\n",
    "\n",
    "    def render(self, depths, density, rgb):\n",
    "        \"\"\"Compute the final rendered color given the depths, density, and RGB values.\"\"\"\n",
    "        distances = depths[..., 1:] - depths[..., :-1]\n",
    "        \n",
    "        # Add extra dimensions to distances for broadcasting\n",
    "        extra_dims = len(density.shape) - len(distances.shape)\n",
    "        for _ in range(extra_dims):\n",
    "            distances = distances[..., None]\n",
    "\n",
    "        density_delta = density * distances\n",
    "        alpha = 1 - torch.exp(-density_delta)\n",
    "        trans = torch.exp(-torch.cat([torch.zeros_like(density_delta[..., :1]), \n",
    "                                      torch.cumsum(density_delta[..., :-1], dim=-1)], dim=-1))\n",
    "\n",
    "        weights = alpha * trans\n",
    "        color_acc = torch.einsum('...i,...ij->...j', weights, rgb)\n",
    "        acc = weights.sum(dim=-1, keepdim=True)\n",
    "\n",
    "        return color_acc + (1.0 - acc) * torch.tensor(self.background)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Renderer` class is designed to \"render\" or create 2D images from 3D data, specifically \"density\" and \"color\". It has methods for sampling points along rays and for rendering a final image.\n",
    "\n",
    "#### `__init__` Method\n",
    "\n",
    "When you create an object of this class, this method sets up all the basic parameters like the number of samples to consider (`num_samples`), the near and far limits (`near` and `far`), and the background color (`background`).\n",
    "\n",
    "#### `sample_along_ray` Method\n",
    "\n",
    "Given starting points (`origins`) and directions (`directions`) for rays in 3D space, this method calculates where to sample along each ray. It returns the \"depths\" (distance from the origin) and \"samples\" (the actual 3D coordinates of these points).\n",
    "\n",
    "#### `render` Method\n",
    "\n",
    "This method takes in \"depths,\" \"density,\" and RGB color values for the sampled points and calculates what the final color of each ray should be.\n",
    "\n",
    "Here's a bit more detail on some specific steps:\n",
    "\n",
    "1. **Distances between adjacent samples:** It calculates the distances between adjacent depths (points we sampled along the ray).\n",
    "\n",
    "2. **Density Delta:** It calculates `density_delta`, which is essentially how much \"stuff\" is between each adjacent sample. This is used to understand how the light interacts with the material as it passes through.\n",
    "\n",
    "3. **Alpha and Transmittance:** It calculates how much light gets absorbed (`alpha`) and how much gets transmitted (`trans`) at each sample along the ray.\n",
    "\n",
    "4. **Weights:** Combines `alpha` and `trans` to form the \"weights\" for each sample, which dictate how much each sample contributes to the final color.\n",
    "\n",
    "5. **Final Color:** The final color (`color_acc`) for each ray is calculated using these weights and the given RGB values.\n",
    "\n",
    "6. **Background Mixing:** The function finally mixes the calculated color with the background color, taking into account how much each ray is occluded by objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize renderer\n",
    "renderer = Renderer()\n",
    "\n",
    "# Create an example set of 2x2 ray origins and directions\n",
    "ray_origins = torch.tensor([[[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]],\n",
    "                            [[0.7, 0.8, 0.9], [0.1, 0.2, 0.3]]])\n",
    "\n",
    "ray_directions = torch.tensor([[[0.3, 0.2, 0.1], [0.6, 0.5, 0.4]],\n",
    "                               [[0.9, 0.8, 0.7], [0.3, 0.2, 0.1]]])\n",
    "\n",
    "# Sample points along the rays\n",
    "depths, samples = renderer.sample_along_ray(ray_origins, ray_directions)\n",
    "print(\"Depth tensor shape:\", depths.shape)\n",
    "print(\"Samples tensor shape:\", samples.shape)\n",
    "\n",
    "# Take these and render\n",
    "density = torch.ones((2, 2, 32))\n",
    "rgb = torch.ones((2, 2, 32, 3))\n",
    "predicted = renderer.render(depths, density, rgb)\n",
    "\n",
    "# Verify shape of the output\n",
    "print(\"Predicted tensor shape:\", predicted.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define a simple DVGO (Direct Voxel Grid Optimization, see XXX et al.) class by inheriting from this renderer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleDVGO(nn.Module):\n",
    "    def __init__(self, shape=(128, 128, 128), min_corner=(-1, -1, -1), max_corner=(1, 1, 1)):\n",
    "        \"\"\"Initialize voxel grids and bounding box corners.\"\"\"\n",
    "        super(SimpleDVGO, self).__init__()\n",
    "        self.shape = shape\n",
    "        self.renderer = Renderer()\n",
    "        self.rgb_voxel_grid = VoxelGrid(self.shape, d=3)\n",
    "        self.density_voxel_grid = VoxelGrid(self.shape, d=1)\n",
    "        self.min_corner = torch.tensor(min_corner, dtype=torch.float32)\n",
    "        self.max_corner = torch.tensor(max_corner, dtype=torch.float32)\n",
    "\n",
    "    def grid_from_scene(self, p):\n",
    "        \"\"\"Rescale coordinates based on the grid dimensions.\"\"\"\n",
    "        scale = 1.0 / (self.max_corner - self.min_corner)\n",
    "        unclamped = (p - self.min_corner) * scale\n",
    "        return torch.clamp(unclamped, 0.0, 0.9999999) * torch.tensor(self.shape).float()\n",
    "        \n",
    "    def forward(self, x_samples):\n",
    "        \"\"\"Perform volume rendering using the provided ray information.\"\"\"\n",
    "        # Extract ray origins and directions from x_samples\n",
    "        origins = x_samples[..., :3]\n",
    "        directions = x_samples[..., :3]\n",
    "\n",
    "        # Sample along the ray\n",
    "        depths, samples = self.renderer.sample_along_ray(origins, directions)\n",
    "\n",
    "        # Rescale to fit within the grid\n",
    "        rescaled = self.grid_from_scene(samples)\n",
    "\n",
    "        # Query Density Voxel Grid\n",
    "        density = F.softplus(torch.squeeze(self.density_voxel_grid(rescaled)))\n",
    "        \n",
    "        # Query RGB Voxel Grid\n",
    "        rgb = torch.sigmoid(self.rgb_voxel_grid(rescaled))\n",
    "\n",
    "        # Render\n",
    "        return self.renderer.render(depths, density, rgb)\n",
    "        return y_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your SimpleDVGO model\n",
    "model = SimpleDVGO()\n",
    "\n",
    "# Combine ray origins and directions into one tensor to use as x_samples\n",
    "# Concatenate along the last dimension (-1)\n",
    "x_samples = torch.cat((ray_origins, ray_directions), dim=-1)\n",
    "print(\"x_samples shape:\", x_samples.shape)\n",
    "\n",
    "predicted = model(x_samples)\n",
    "assert predicted.shape == (2, 2, 3)\n",
    "\n",
    "# Create a corresponding set of 3 target RGB colors\n",
    "# Each one is a 3D RGB color; assume they're randomly initialized here\n",
    "y_samples = torch.tensor([[[0.3, 0.2, 0.1], [0.6, 0.5, 0.4]],\n",
    "                          [[0.9, 0.8, 0.7], [0.3, 0.2, 0.1]]])\n",
    "\n",
    "# Train the model\n",
    "train(model, x_samples, y_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a NeRF with images\n",
    "\n",
    "Now that we built the basic infrastructure to render a NeRF and regress it from data, let us apply it to real images. To do this, we will need two distinct pieces of information:\n",
    "\n",
    "- the actual images themselves\n",
    "- accurate geometry from where the images were taken\n",
    "\n",
    "The latter is important because, as we saw above, a NeRF is trained with a set of *rays*. For a given image, every pixel in the image corresponds to a ray. The *origin* of the ray is exactly the optical center of the camera used to acquire the image, so we need at least that information. But to calculate, for every pixel, the *direction* of the ray needs a lot more. In fact, this is typically a two step process:\n",
    "\n",
    "- the *intrinsic* calibration of the camera, most importantly the focal length, tells us how to convert pixel coordinates into a direction in the camera frame.\n",
    "- the *extrinsic* calibration, position and orientation with which the image was taken, is needed to transform directions in the camera frame into the scene coordinate frame.\n",
    "\n",
    "Acquiring all this information for an arbitrary image sequence taken with some unknown, uncalibrated camera can be complicated. Cameras come in a variety of sizes and with very different lenses, and effects like radial distortion make the modeling process non-trivial. In addition, recovering the actual position and attitude of the camera in a scene is typically done through structure from motion, which can be time-consuming and tricky."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the following, we will assume that all this hard work has been done for us, and/or the images have been simulated with exactly known camera parameters, both intrinsic and extrinsic. One popular way to accomplish this is by proving undistorted images accompanied by a $3 \\times 4$ **camera matrix** $M$. Recall that a 3D point $P$ can be projected into an image via\n",
    "\n",
    "$$\n",
    "\\tilde{p} = K R^T (P - t)\n",
    "$$\n",
    "\n",
    "where $\\tilde{p}$ are *homogeneous$ 2D image coordinates. We can re-write this as\n",
    "\n",
    "$$\n",
    "\\tilde{p} = M\\tilde{P}\n",
    "$$\n",
    "\n",
    "where $\\tilde{P} = \\begin{bmatrix}P \\\\1 \\end{bmatrix}$ and the camera matrix $M$ is given by\n",
    "\n",
    "$$\n",
    "M = [A|a] = [K R^T | - K R^T t]\n",
    "$$\n",
    "\n",
    "That means that if we are *given* the camera matrix $M$ we can always recover the ray origins as \n",
    "$$\n",
    "t = -A^{-1} a\n",
    "$$\n",
    "\n",
    "and a random 3D point $P$ on the ray corresponding to $\\tilde{p}$ as\n",
    "\n",
    "$$\n",
    "P = A^{-1}(\\tilde{p} - a)\n",
    "$$\n",
    "\n",
    "since $\\tilde{p} = AP + a$. We will illustrate this below with an actual dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simulated dataset\n",
    "\n",
    "<figure>\n",
    "<img src=\"stonehenge/train/render47.png\" id=\"fig:stonehenge\" style=\"width:14cm\" alt=\"\">\n",
    "<figcaption>One of the synthetically generated images from the stonehenge dataset.</figcaption>\n",
    "</figure>\n",
    "\n",
    "In particular, we will use a dataset generated by Stanford researchers using Blender, using a simple 3D model of the Stonehenge monument in England. The dataset was originally published as part of the [Stanford NeRF Navigation project](https://mikh3x4.github.io/nerf-navigation/), and consists of 500 images, split into 200 images for training, and then validation and tests sets of 150 images each. One of the training images is shown above. The code below illustrates how to read the image into memory and display it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_training_image(index:int):\n",
    "    \"\"\"Read image from the stonehenge dataset, and return as a PIL image.\"\"\"\n",
    "    # url = f\"https://raw.githubusercontent.com/gtbook/robotics/main/Figures5/{image_name}\"\n",
    "    # return PIL.Image.open(requests.get(url, stream=True).raw)\n",
    "    return PIL.Image.open(f\"stonehenge/train/render{index}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = read_training_image(47)\n",
    "plt.imshow(image);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to modify the index above and examine any of the other 198 images in the training set. The camera matrices are all given in a json file, which we can parse into a python dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the local JSON file and read its content\n",
    "with open(\"stonehenge/transforms_train.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Now, `data` contains the parsed JSON content.\n",
    "print(data)  # For verification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The camera matrix associated with the image below can then be extracted by converting to numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_intrinsics(image_size: tuple, camera_angle_x: float) -> np.ndarray:\n",
    "    \"\"\"Calculate the intrinsic matrix given the image size and camera angle.\"\"\"\n",
    "    W, H = image_size\n",
    "    f = W / (2 * np.tan(camera_angle_x/2))\n",
    "    return np.array([[f, 0, W/2], [0, f, H/2], [0, 0, 1]])\n",
    "\n",
    "def extract_extrinsics(data: dict, index: int) -> np.ndarray:\n",
    "    \"\"\"Extract the extrinsic matrix from the given data.\"\"\"\n",
    "    wTc = np.array(data[\"frames\"][index][\"transform_matrix\"]) # Make sure to use the index parameter\n",
    "    t = wTc[:3, 3] # translation\n",
    "    R = wTc[:3, :3] # rotation\n",
    "    return np.hstack((R.T, -R.T @ t.reshape(-1, 1)))\n",
    "\n",
    "def read_json_data(filepath: str) -> dict:\n",
    "    \"\"\"Read the JSON data from a given filepath.\"\"\"\n",
    "    with open(filepath, \"r\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def read_camera_matrix(index:int, image_size: tuple) -> np.ndarray:\n",
    "    \"\"\"Read the 3x4 camera matrix associated with a training image.\"\"\"\n",
    "    data = read_json_data(\"stonehenge/transforms_train.json\")\n",
    "    K = calculate_intrinsics(image_size, data['camera_angle_x'])\n",
    "    M = extract_extrinsics(data, index)\n",
    "    return K @ M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = read_camera_matrix(47, image.size)\n",
    "print(M.shape)\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_json_data(\"stonehenge/transforms_train.json\")\n",
    "calculate_intrinsics(image.size, data['camera_angle_x'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then calculate the ray origin and ray direction for any pixel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, a = M[:, :3], M[:, 3]\n",
    "t = -np.linalg.inv(A) @ a\n",
    "p_ = np.array([2.5, 1.5, 1])\n",
    "P = np.linalg.inv(A) @ (p_ - a)\n",
    "D = P / np.linalg.norm(P)\n",
    "print(t, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rays(M: np.ndarray, image_size: tuple) -> tuple:\n",
    "    \"\"\"Calculate a batch of rays associated with every pixel in a given image.\"\"\"\n",
    "    W, H = image_size\n",
    "    A, a = torch.from_numpy(M[:, :3]), torch.from_numpy(M[:, 3])\n",
    "    inv_A = torch.inverse(A)\n",
    "\n",
    "    # Compute origin and expand to all pixels.\n",
    "    t = -inv_A @ a\n",
    "    ones = torch.ones((H, W))\n",
    "    T = torch.einsum('i,hw->hwi', t, ones) # creates (H,W,3) batch\n",
    "    \n",
    "    # Batch compute directions for all pixels.\n",
    "    x, y = torch.meshgrid(torch.linspace(0.5, W-0.5, W), torch.linspace(0.5, H-0.5, H), indexing='xy')\n",
    "    p_ = torch.stack([x, y, ones], dim=-1)\n",
    "    P = torch.einsum('ij,hwj->hwi', inv_A, p_ - a) # batch matrix multiply\n",
    "\n",
    "    norms = -torch.linalg.norm(P, dim=-1, keepdim=True)\n",
    "    D = P / (norms + 1e-8)\n",
    "\n",
    "    return T, D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this for a simple 4x3 image (where 4 is width and 3 is height), and check that the batch dimensions are as we expect (height, width, 3), and that for the center pixel they agree with our manual calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origins, directions = calculate_rays(M, (5, 3))\n",
    "assert origins.shape == (3, 5, 3)\n",
    "assert directions.shape == (3, 5, 3)\n",
    "assert np.allclose(origins[1, 2, :], t)\n",
    "assert np.allclose(directions[1, 2, :], D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "for i in range(0, 199, 10):\n",
    "    M = read_camera_matrix(i, image.size)\n",
    "    origins, directions = calculate_rays(M, image.size)\n",
    "    T = origins[::100,::100,:].reshape(-1, 3)\n",
    "    D = directions[::100,::100,:].reshape(-1, 3)\n",
    "\n",
    "    # Adding line segments for each ray\n",
    "    for start, end in zip(T, T + D):\n",
    "        fig.add_trace(go.Scatter3d(x=[start[0], end[0]],\n",
    "                                y=[start[1], end[1]],\n",
    "                                z=[start[2], end[2]],\n",
    "                                mode='lines',\n",
    "                                line=dict(color='red')))\n",
    "\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "S76_drone_learning.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "latex_metadata": {
   "affiliation": "Georgia Institute of Technology",
   "author": "Frank Dellaert and Seth Hutchinson",
   "title": "Introduction to Robotics"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
