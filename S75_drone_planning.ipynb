{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github",
    "tags": [
     "no-tex"
    ]
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gtbook/robotics/blob/main/S75_drone_decision_theory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JoW4C_OkOMhe",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%pip install -q -U gtbook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10-snNDwOSuC",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.subplots as pls\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "import gtsam\n",
    "from gtsam import Point2\n",
    "\n",
    "from gtbook import diffdrive\n",
    "from gtbook.discrete import Variables\n",
    "from gtbook.display import show, pretty\n",
    "\n",
    "import torch\n",
    "DEVICE = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "rng = np.random.default_rng()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def create_random_map(W, H, num_obstacles=50, seed=42):\n",
    "    \"\"\"\n",
    "    Creates a random occupancy map with the given dimensions and number of obstacles.\n",
    "    Optionally, a seed can be provided to make the map reproducible.\n",
    "    \"\"\"\n",
    "    # Set the seed for reproducibility:\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "    # Map dimensions in pixels (10 cm per pixel)\n",
    "    pixel_W, pixel_H = 10*W, 10*H\n",
    "\n",
    "    # Initialize an empty occupancy map\n",
    "    obstacles = torch.zeros((pixel_H, pixel_W), dtype=torch.float32)\n",
    "\n",
    "    # Add obstacles\n",
    "    for _ in range(num_obstacles):\n",
    "        # Random position for the obstacle\n",
    "        x, y = rng.integers(0, pixel_W), rng.integers(0, pixel_H)\n",
    "\n",
    "        # Random value for the obstacle (higher than empty space)\n",
    "        obstacle_value = rng.random() * 0.5 + 0.5  # Random value between 0.5 and 1.0\n",
    "\n",
    "        # Assign the value to the obstacle position\n",
    "        obstacles[y, x] = obstacle_value\n",
    "\n",
    "    # Define a box filter (kernel)\n",
    "    kernel_size = 5  # The size of the box filter\n",
    "    kernel = torch.ones((kernel_size, kernel_size), dtype=torch.float32)\n",
    "\n",
    "    # Apply 2D convolution to \"fatten\" the objects\n",
    "    batch = obstacles[None, None, ...]  # Add batch and channel dimensions\n",
    "    return torch.conv2d(batch, kernel[None, None, ...], padding='same')[0, 0, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def displaced_gaussian(\n",
    "    sigma: float, K: int, uv: np.ndarray\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns a Gaussian kernel of size K x K with standard deviation sigma.\n",
    "    The kernel is centered at uv, a float coordinate in xy convention.\n",
    "    Also returns the indices of the kernel in the image.\n",
    "    \"\"\"\n",
    "    # Create meshgrid around uv, in ij convention\n",
    "    j0, i0 = np.round(uv).astype(int)\n",
    "    kk = np.arange(-(K // 2), K // 2 + 1)\n",
    "    ii, jj = np.meshgrid(i0 + kk, j0 + kk, indexing=\"ij\")\n",
    "\n",
    "    # Calculate the kernel\n",
    "    dd = (ii - uv[1]) ** 2 + (jj - uv[0]) ** 2\n",
    "    kernel = np.exp(-dd / (2 * sigma**2))\n",
    "    kernel = kernel / kernel.sum()\n",
    "\n",
    "    # Return the kernel and the indices of the kernel in the image\n",
    "    return kernel, ii, jj\n",
    "\n",
    "\n",
    "def gaussian_kernel(sigma: float, K: int = 9) -> torch.Tensor:\n",
    "    \"\"\"Generates a Gaussian kernel of shape (1, 1, K, K) with standard deviation sigma.\"\"\"\n",
    "    uv = np.array([K // 2, K // 2])\n",
    "    numpy_kernel, _, _ = displaced_gaussian(sigma, K, uv)\n",
    "    return torch.tensor(numpy_kernel, dtype=torch.float32)[None, None, ...]\n",
    "\n",
    "\n",
    "def gaussian_filter(sigma, uv, image, K=9):\n",
    "    \"\"\"Applies a Gaussian filter at uv on the given image.\"\"\"\n",
    "    kernel, ii, jj = displaced_gaussian(sigma, K, uv)\n",
    "    \n",
    "    # Dimensions of the image\n",
    "    height, width = image.shape[-2:]\n",
    "\n",
    "    # Find the valid range for ii and jj\n",
    "    valid_mask = (ii >= 0) & (ii < height) & (jj >= 0) & (jj < width)\n",
    "    valid_ii = ii[valid_mask]\n",
    "    valid_jj = jj[valid_mask]\n",
    "\n",
    "    # Adjust the kernel to match the valid range\n",
    "    valid_kernel = kernel[valid_mask]\n",
    "\n",
    "    # Extract the relevant portion of the image\n",
    "    image_excerpt = image[..., valid_ii, valid_jj].numpy()\n",
    "\n",
    "    # Apply the valid kernel to the image excerpt\n",
    "    return (valid_kernel * image_excerpt).sum(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def sobel_kernels(dtype=torch.float32, **kwargs):\n",
    "    \"\"\"Return Sobel gradient kernels sobel_u of shape (1, 1, 1, 3) and sobel_v of shape (1, 1, 3, 1).\"\"\"\n",
    "    sobel_u = torch.tensor([[-1, 0, 1]], dtype=dtype, **kwargs)\n",
    "    sobel_v = sobel_u.T\n",
    "    return sobel_u[None, None, ...], sobel_v[None, None, ...]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "nAvx4-UCNzt2"
   },
   "source": [
    "# Trajectory Optimization\n",
    "\n",
    "> We can use factor graphs to optimize over future trajectories, as well.\n",
    "\n",
    "<img src=\"Figures7/S75-Autonomous_camera_drone-05.jpg\"  alt=\"Splash image with steampunk drone in aggressive maneuver\" width=\"40%\" align=center style=\"vertical-align:middle;margin:10px 0px\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section we saw how do use factor graphs for visual SLAM and structure from motion. These perception algorithms are typically run after we have gathered some visual information, and inform us about what happened in the past. But how can we plan for the future? We already saw that RRTs are useful paradigm to plan in a continuous, potentially high dimensional state space. \n",
    "\n",
    "In this section we will discuss a different method, which is optimal control. In particular, we will use **trajectory optimization**, to minimize the cost associated with the trajectory that we would like to execute in the future. These costs can be associated with staying away from obstacles, or other desirable properties like minimizing power consumption to preserve battery life. Finally, RRTs and trajectory optimization can be combined, where RRT finds a good \"broad strokes\" trajectory, and trajectory optimization smooths out and fine-tunes the final trajectory."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Objective\n",
    "\n",
    "We explain trajectory optimization using a very simple problem: how to get from point A to point B. This seems like an easy problem, as we all know the answer should be a straight line, right? But, it is not as easy for drones, as we have to account for the drone's attitude, and we might have to accelerate and decelerate at the start and end points. Come to think of it, maybe we are already flying at a certain velocity at point A, and want to have a certain velocity at point B! Clearly, this is not as easy as it appears.\n",
    "\n",
    "There are many ways to go about this:\n",
    "- MPC with collocated polynomials\n",
    "- discrete poses and controls + dynamics factors\n",
    "- differential flatness + controller: this is what we'll do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing for Position\n",
    "\n",
    "It turns out we can optimize for position only, without worrying about attitude, at least at first. This is not a trivial statement, and it is also very fortunate. Let us assume that we want to fly in a forest, full of trees, and we want a trajectory for the drone avoid the trees. A quadrotor-style drone trajectory has the potential to be rather complicated: the drone lives in the 6DOF space of position (3DOF) and attitude (3DOF). And we know from Section 7.2 that attitude will *affect* position: if we pitch or role, that will affect our velocity, and that will integrate into position. That very mechanism is also a source of complexity: a drone is *under-actuated*: we cannot directly control the 6DOF, we have to do it through the action only four rotors. It seems like a very complex problem indeed.\n",
    "\n",
    "When given a continuous trajectory of position that also behaves well in terms of velocity, we can obtain the pitch, roll, and thrust trajectories to obtain it. Of course, it can't be a totally crazy trajectory: a drone can't stop on a dime, or teleport, or go faster than certain limits. But if we have a 3DOF translational trajectory that is well behaved, we can \"upgrade\" it to a full position/attitude in two different ways: in advance, by solving a large optimization problem, or physically, in real-time, by using a tracking controller. The latter is what we will do, after obtaining a trajectory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find a suitable translational trajectory that avoids obstacles, we will use optimization. There are many ways one can do this, some better than others, but here strive for simplicity: we discretize the time interval in which we expect to fly and solve for a set of corresponding set of positions over time that satisfy our objectives. These objectives can include:\n",
    "\n",
    "- a desired starting position (and velocity, if so desired);\n",
    "- a desired goal position (and velocity, if desired);\n",
    "- a desired distance from the closest obstacle at any time;\n",
    "- a smoothness objective that bounds our velocity and/or acceleration.\n",
    "\n",
    "If these objectives are in conflict, we can assign them weights such that we trade off some against others. Constrained optimization techniques allow to set hard constraints and/or bounds, but we do not use those here. Instead, we opt for a simple non-linear least squares scheme that minimizes the following objective:\n",
    "\n",
    "$$\n",
    "X^* = \\arg \\min \\sum_{k=1}^K \\phi_k(X_k)^2  + \\sum_{k=1}^{K-1} \\psi_k(X_k, X_{k+1})^2\n",
    "$$\n",
    "\n",
    "where $X_k$ is the position at time, and $\\phi_k(X_k)$ and $\\psi_k(X_k, X_{k+1})$ are unary and binary objectives, respectively, that we want to minimize. Desired start and goal, as well as obstacle avoidance are examples of unary objectives, while smoothness is a binary objective, as it evaluates the distance/velocity between successive positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will not come as any surprise that we can also use factor graph optimization to find our optimal trajectories. The objective above is exactly the expression of a nonlinear factor graph, except that the factors now derive from objectives we want to minimize, rather than measurement errors. In the next section we work in detail how to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Occupancy and Cost Maps\n",
    "\n",
    "A very popular approach to represent environments with different obstacles is to use an **occupancy map**. An occupancy map is essentially a 2D array of cells, each representing the environment at a certain resolution (for example, 10 cm by 10 cm). Each cell in this map contains a probability for the presence of an obstacle. This concept is further extendable to distinguish between various types of obstacles, each with different levels of lethality. This extended version can then be transformed into a **cost map**, where areas with less hazardous obstacles (like tall grass) are considered less costly compared to more dangerous ones (like flying into a tree). For our purposes, we will start directly with cost maps instead of creating true occupancy maps.\n",
    "\n",
    "To illustrate, let us consider creating a simple example cost map. In the `gtbook` library, we have a function named `create_random_map`. This function requires as parameters the width $W$, height $H$, and the number of obstacles to create a random arrangement of obstacles within the map. Width and height are given in meters, and the resolution is hardcoded at 10 cm. An optional parameter is a random seed, which allows for the reproduction of the exact experiment setup. The following code snippet demonstrates how to generate a random map with 50 obstacles using a random seed of 42:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random cost map\n",
    "W, H = 30, 10  # 30m x 10m\n",
    "cost_map = create_random_map(W, H, num_obstacles=50, seed=7)\n",
    "px.imshow(cost_map, color_continuous_scale='Reds').show()\n",
    "#| caption: Cost map with randomly generated obstacles, at 10cm resolution.\n",
    "#| label: fig:obstacles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create differentiable factors $\\phi_k(X_k)$ that avoid high cost areas, we can smooth the cost map with a differentiable kernel, like a Gaussian. The cost map above is a torch tensor, representing a grayscale image 100 pixels tall and 300 pixels wide. Convolving an image with a kernel is relatively easy in pytorch, though we need to juggle some tensor dimensions because by convention, `conv2d` operates on $\\text{batch} \\times \\text{channel} \\times \\text{height} \\times \\text{width}$ tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0.5  # 0.5m standard deviation for the Gaussian kernel\n",
    "K = 21 # 21x21 kernel is big enough to accommodate that standard deviation \n",
    "kernel = gaussian_kernel(sigma*10, K) # multiply by 10 as map is 10cm resolution\n",
    "batch = cost_map[None, None, ...]  # Add batch and channel dimensions\n",
    "blurred = torch.conv2d(batch, kernel, padding='same')[0, 0, ...]\n",
    "px.imshow(blurred, color_continuous_scale='Reds').show()\n",
    "#| caption: Cost map blurred with 0.5 meter Gaussian kernel.\n",
    "#| label: fig:blurred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get truly differentiable factors, however, it is better to construct a custom Gaussian kernel at the exact location we want to evaluate the cost. Remember we are trying to minimize a trajectory, by moving around a set of positions $X_k$ at discrete time stamps $t_k$. In non-linear optimization, the story is always the same: evaluate the cost *and* its derivatives at a current guess, and then use the derivatives to move to a lower cost solution. However, if we smooth a discretized cost map, the result is still discretized, however smooth it looks to us. To get the *exact* value of the cost, and its derivatives, we can create a Gaussian kernel centered *exactly* at our current *continuous* position.\n",
    "\n",
    "The function `gaussian_filter` does just that: it evaluates a single Gaussian kernel operator given a standard deviation, a continuous map location, and the map to operate on. The code below shows that this gives the exact same result as a lookup in the blurred image when given an integer coordinate, but it *also* works for non-integer coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = Point2(5.9, 1.1) # point in map, in meters\n",
    "uv = 10*xy # continuous position in image\n",
    "local_result = gaussian_filter(sigma*10, uv, cost_map, K)\n",
    "print(f\"Local cost at {xy} is {local_result:.3f}\")\n",
    "\n",
    "# When uv are at integer values, blurred image gives the same result:\n",
    "assert np.allclose(local_result, blurred[int(uv[1]), int(uv[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the derivatives we blur the vertical and horizontal gradient images, respectively. Remember that we need the derivative of the exact cost with respect to x and y, if we are to optimize over those positions. Because convolution is a linear operation, we can take the derivative of the Gaussian kernel with respect to x and y, *or* we can apply the convolution on a derivative image. We choose the latter, by first using the Sobel operator we encountered in Section 5.4 to create gradient images: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients:\n",
    "sobel_u, sobel_v = sobel_kernels()\n",
    "grad_u = torch.conv2d(batch, sobel_u, padding='same')[0,0,...]\n",
    "grad_v = torch.conv2d(batch, sobel_v, padding='same')[0,0,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "#| caption: Obstacle cost map, blurred cost map, and both vertical and horizontal sobel gradients.\n",
    "#| label: fig:obstacles-2x2\n",
    "fig = pls.make_subplots(rows=2, cols=2)\n",
    "fig.add_trace(go.Heatmap(z=cost_map, colorscale='reds', showscale=False), row=1, col=1)\n",
    "fig.add_trace(go.Heatmap(z=blurred, colorscale='reds', showscale=False), row=1, col=2)\n",
    "fig.add_trace(go.Heatmap(z=grad_u, colorscale='rdbu', showscale=False), row=2, col=1)\n",
    "fig.add_trace(go.Heatmap(z=grad_v, colorscale='rdbu', showscale=False), row=2, col=2)\n",
    "for i in [1,2]:\n",
    "    for j in [1,2]:\n",
    "        fig.update_xaxes(title_text=\"\", showticklabels=False, row=i, col=j)\n",
    "        fig.update_yaxes(title_text=\"\", showticklabels=False, row=i, col=j)\n",
    "fig.update_layout(margin=dict(l=10, r=10, t=10, b=10))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure <a href=\"#fig:obstacles-2x2\" data-reference-type=\"ref\" data-reference=\"fig:obstacles-2x2\">3</a> shows both cost map and its blurred version, as well as the (un-blurred) gradient images `grad_u` and `grad_v`. We can then combined these into one rank-3 tensor and evaluate the cost *and* derivatives in one operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = torch.stack([cost_map, grad_u, grad_v], dim=0)\n",
    "print(np.round(gaussian_filter(sigma*10, uv, combined, K),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor Graphs for Trajectory Optimization\n",
    "\n",
    "Now that we can evaluate cost its derivatives at any location, we can create factors. Since occupancy maps or cost maps are not built into GTSAM, we use its facility to create a custom factor from arbitrary python code. A `gtsam.CustomFactor` just has a constructor that can take an arbitrary python callback function, along with a `Key` (and a noise model). At evaluation, the callback function gets a handle to the factor and a `gtsam.Values` object, and it is supposed to do three things:\n",
    "\n",
    "- check which variable is involved, by asking the factor;\n",
    "- with that key, extract the current estimate from the passed in `Values`;\n",
    "- calculate the cost, and if asked, its derivatives.\n",
    "\n",
    "The fact that we can do this for arbitrary python code is very convenient, and is used below to create cost factors that interrogate the cost map, using our localized Gaussian filter, and if asked, its derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_func(factor: gtsam.CustomFactor, v: gtsam.Values, H: list[np.ndarray]):\n",
    "    \"\"\"Error function for the custom factor.\"\"\"\n",
    "    # Extract the Point2 variable using the key in the factor\n",
    "    point = v.atPoint2(factor.keys()[0])\n",
    "\n",
    "    uv = 10 * point # Convert to map coordinates\n",
    "\n",
    "    if H is None:\n",
    "        cost = gaussian_filter(sigma * 10, uv, cost_map, K)\n",
    "    else:\n",
    "        # If Jacobian is needed, calculate them here:\n",
    "        cost, cost_x, cost_y = gaussian_filter(sigma * 10, uv, combined, K)\n",
    "        H[0] = np.array([[cost_x, cost_y]])\n",
    "\n",
    "    return np.array([cost])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can immediately create all obstacle cost factors now. Let us discretize the trajectory with 100 factors, using the keys $k\\in 1...100$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = gtsam.NonlinearFactorGraph()\n",
    "for k in range(1,101):\n",
    "    custom_factor = gtsam.CustomFactor(None, [k], error_func)\n",
    "    graph.add(custom_factor)\n",
    "print(f\"Graph has {graph.size()} factors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the last one we created, with key $k=100$, at the point `xy` that we already used as a test case above. By inspecting the result, you can see that the cost and derivatives match our earlier example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Values and evaluate the factor and its Jacobian:\n",
    "values = gtsam.Values()\n",
    "values.insert(100, Point2(5.9, 1.1))\n",
    "error = custom_factor.error(values)\n",
    "print(f\"Cost: {error}\")\n",
    "linear = custom_factor.linearize(values)\n",
    "print(f\"Jacobian: {linear.jacobian()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also add start and goal factors, as well as smoothness factors. Both start and goal factors can be implemented as simple priors on the variables with keys $k=1$ and $k=100$. Smoothness factors are implemented using a `gtsam.BetweenFactor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, goal = Point2(2, 5), Point2(28, 5)\n",
    "prior_model = gtsam.noiseModel.Constrained.All(2)\n",
    "prior_model = gtsam.noiseModel.Isotropic.Precision(2, 1)\n",
    "graph.addPriorPoint2(1, start, prior_model)\n",
    "graph.addPriorPoint2(100, goal, prior_model)\n",
    "smoothness_model = gtsam.noiseModel.Isotropic.Precision(2, 0.01)\n",
    "for k in range(1, 100):\n",
    "    graph.add(gtsam.BetweenFactorPoint2(k, k+1, gtsam.Point2(0.0, 0.0), smoothness_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a straight line initial estimate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial = gtsam.Values()\n",
    "for k in range(1,101):\n",
    "    alpha = (k-1)/100\n",
    "    initial.insert(k, (1-alpha)*start + alpha*goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And optimize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = gtsam.LevenbergMarquardtParams()\n",
    "params.setVerbosityLM(\"SUMMARY\")\n",
    "params.setlambdaInitial(0.1)\n",
    "optimizer = gtsam.LevenbergMarquardtOptimizer(graph, initial, params)\n",
    "result = optimizer.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract points from result Values:\n",
    "initial_path = 10*gtsam.utilities.extractPoint2(initial)\n",
    "result_path = 10*gtsam.utilities.extractPoint2(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot result on the cost map:\n",
    "fig = px.imshow(blurred, color_continuous_scale='Reds')\n",
    "fig.add_trace(go.Scatter(x=initial_path[:,0], y=initial_path[:,1], mode='lines+markers', line=dict(color='gray')))\n",
    "fig.add_trace(go.Scatter(x=result_path[:,0], y=result_path[:,1], mode='lines+markers', line=dict(color='green')))\n",
    "fig.update_layout(coloraxis_showscale=False, showlegend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing\n",
    "\n",
    "- A magic Chebyshev interpolation class that gets us derivatives and accelerations\n",
    "- Can also be used for RRT, see project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translational Control\n",
    "\n",
    "- A thrust controller\n",
    "- A pitch/roll controller\n",
    "  - calculate desired acceleration\n",
    "  - from that we calculate thrust vector we need, compensate for gravity\n",
    "  - project onto the F and L body axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planning and Controlling Yaw\n",
    "\n",
    "- We'd like to point in a particular direction\n",
    "- forward looking, or camera drone\n",
    "- Explain why controlling yaw is harder\n",
    "- A simple yaw controller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "S75_drone_decision_theory.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('gtbook')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "latex_metadata": {
   "affiliation": "Georgia Institute of Technology",
   "author": "Frank Dellaert and Seth Hutchinson",
   "title": "Introduction to Robotics"
  },
  "vscode": {
   "interpreter": {
    "hash": "9f7376ced4243bb13dfcffa8a3ba834e0602aa8334cd3a1d8ba8d285f4628083"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
